{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR Agent System - Comprehensive Test Suite\n",
    "\n",
    "This notebook provides comprehensive testing for the PR Agent System, validating all core functionality:\n",
    "\n",
    "1. **Configuration Testing** - Validate environment setup\n",
    "2. **Profile Management** - Test executive profile operations\n",
    "3. **Synchronous Workflow** - Test basic comment generation\n",
    "4. **Asynchronous Workflow** - Test async/parallel operations\n",
    "5. **Streaming Workflow** - Test real-time streaming\n",
    "6. **Phase 3 Features** - Test memory, RAG, and evaluation\n",
    "7. **Error Handling** - Test resilience and fallbacks\n",
    "8. **Cache Testing** - Test caching behavior\n",
    "\n",
    "**Note**: This notebook requires proper API keys configured in your `.env` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('../..')\n",
    "\n",
    "from pr_agent import PRCommentAgent, PRAgentConfig\n",
    "from pr_agent.profile_manager import ExecutiveProfileManager\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../../.env')  # Load from project root\n",
    "\n",
    "print(\" Imports successful\")\n",
    "print(f\" Python version: {sys.version}\")\n",
    "print(f\" Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Configuration Validation\n",
    "\n",
    "Verify that configuration is properly set up and all required environment variables are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_configuration():\n",
    "    \"\"\"Test configuration setup and validation.\"\"\"\n",
    "    print(\"Testing Configuration...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test 1.1: Create default config\n",
    "    try:\n",
    "        config = PRAgentConfig()\n",
    "        print(\" Default configuration created\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to create config: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 1.2: Check LLM provider\n",
    "    if config.openai_api_key:\n",
    "        print(f\" OpenAI API key found (model: {config.model_name})\")\n",
    "    elif config.anthropic_api_key:\n",
    "        print(f\" Anthropic API key found (model: {config.model_name})\")\n",
    "    else:\n",
    "        print(\" No LLM API key configured\")\n",
    "        return False\n",
    "    \n",
    "    # Test 1.3: Check search provider\n",
    "    if config.serper_api_key:\n",
    "        print(\" Serper API key found\")\n",
    "    elif config.tavily_api_key:\n",
    "        print(\" Tavily API key found\")\n",
    "    else:\n",
    "        print(\" No search API key configured (some features may fail)\")\n",
    "    \n",
    "    # Test 1.4: Check email configuration\n",
    "    if config.email_from and config.email_password:\n",
    "        print(f\" Email configured: {config.email_from}\")\n",
    "        if config.pr_manager_email:\n",
    "            print(f\" PR manager email: {config.pr_manager_email}\")\n",
    "    else:\n",
    "        print(\" Email not configured (email notifications will be skipped)\")\n",
    "    \n",
    "    # Test 1.5: Validate configuration\n",
    "    try:\n",
    "        config.validate()\n",
    "        print(\" Configuration validation passed\")\n",
    "    except ValueError as e:\n",
    "        print(f\" Configuration validation warnings: {e}\")\n",
    "    \n",
    "    # Test 1.6: Custom configuration\n",
    "    custom_config = PRAgentConfig(\n",
    "        temperature=0.8,\n",
    "        max_search_results=3,\n",
    "        enable_verbose_logging=True\n",
    "    )\n",
    "    assert custom_config.temperature == 0.8\n",
    "    assert custom_config.max_search_results == 3\n",
    "    print(\" Custom configuration works\")\n",
    "    \n",
    "    print(\"\\n Configuration tests passed\\n\")\n",
    "    return True\n",
    "\n",
    "test_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Profile Management\n",
    "\n",
    "Test loading, creating, and managing executive profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_profile_management():\n",
    "    \"\"\"Test executive profile management.\"\"\"\n",
    "    print(\"Testing Profile Management...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    manager = ExecutiveProfileManager()\n",
    "    \n",
    "    # Test 2.1: List profiles\n",
    "    profiles = manager.list_profiles()\n",
    "    print(f\" Found {len(profiles)} profiles: {profiles}\")\n",
    "    \n",
    "    if len(profiles) == 0:\n",
    "        print(\" No profiles found. Create at least one profile to test.\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2.2: Load existing profile\n",
    "    test_profile_name = profiles[0]\n",
    "    try:\n",
    "        profile = manager.load_profile(test_profile_name)\n",
    "        print(f\" Loaded profile: {profile['name']}\")\n",
    "        print(f\"  Title: {profile['title']}\")\n",
    "        print(f\"  Company: {profile.get('company', 'N/A')}\")\n",
    "        print(f\"  Expertise areas: {len(profile.get('expertise', []))}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load profile: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2.3: Validate required fields\n",
    "    required_fields = ['name', 'title', 'communication_style', 'expertise']\n",
    "    for field in required_fields:\n",
    "        if field not in profile or not profile[field]:\n",
    "            print(f\" Missing required field: {field}\")\n",
    "            return False\n",
    "    print(f\" All required fields present\")\n",
    "    \n",
    "    # Test 2.4: Create sample profile\n",
    "    sample_profile = manager.create_sample_profile(\"Test Executive\")\n",
    "    assert sample_profile['name'] == \"Test Executive\"\n",
    "    assert 'expertise' in sample_profile\n",
    "    print(\" Sample profile creation works\")\n",
    "    \n",
    "    # Test 2.5: Profile caching\n",
    "    profile_cached = manager.load_profile(test_profile_name)\n",
    "    assert profile == profile_cached\n",
    "    print(\" Profile caching works\")\n",
    "    \n",
    "    print(\"\\n Profile management tests passed\\n\")\n",
    "    return True\n",
    "\n",
    "test_profile_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Agent Initialization\n",
    "\n",
    "Test that the PR Agent initializes correctly with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_initialization():\n",
    "    \"\"\"Test PR Agent initialization.\"\"\"\n",
    "    print(\"Testing Agent Initialization...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test 3.1: Initialize with default config\n",
    "    try:\n",
    "        config = PRAgentConfig(enable_verbose_logging=False)\n",
    "        agent = PRCommentAgent(config)\n",
    "        print(\" Agent initialized with default config\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to initialize agent: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Test 3.2: Check components\n",
    "    assert agent.main_llm is not None\n",
    "    print(\" Main LLM initialized\")\n",
    "    \n",
    "    assert agent.humanizer_llm is not None\n",
    "    print(\" Humanizer LLM initialized\")\n",
    "    \n",
    "    assert agent.web_search is not None\n",
    "    print(\" Web search tool initialized\")\n",
    "    \n",
    "    assert agent.media_research_tool is not None\n",
    "    print(\" Media research tool initialized\")\n",
    "    \n",
    "    assert agent.email_sender is not None\n",
    "    print(\" Email sender initialized\")\n",
    "    \n",
    "    assert agent.profile_manager is not None\n",
    "    print(\" Profile manager initialized\")\n",
    "    \n",
    "    assert agent.workflow is not None\n",
    "    print(\" Workflow graph built\")\n",
    "    \n",
    "    # Test 3.3: Check specialized agents\n",
    "    assert agent.media_researcher is not None\n",
    "    assert agent.data_researcher is not None\n",
    "    assert agent.comment_drafter is not None\n",
    "    assert agent.humanizer is not None\n",
    "    print(\" All specialized agents initialized\")\n",
    "    \n",
    "    # Test 3.4: Check cache\n",
    "    assert agent.cache is not None\n",
    "    print(f\" Cache initialized (enabled: {agent.cache.enabled})\")\n",
    "    \n",
    "    print(\"\\n Agent initialization tests passed\\n\")\n",
    "    return agent\n",
    "\n",
    "agent = test_agent_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Synchronous Comment Generation\n",
    "\n",
    "Test the basic synchronous workflow for generating PR comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for testing\n",
    "test_article = \"\"\"\n",
    "Recent research from the Global Marketing Institute shows that brands investing \n",
    "in long-term brand building alongside performance marketing see 3x better ROI \n",
    "over a five-year period compared to those focused solely on short-term conversions.\n",
    "\n",
    "The study analyzed $2.3 billion in marketing spend across 150 brands and found \n",
    "that the optimal split is 60% brand building and 40% activation. However, current \n",
    "industry practice shows most brands do the opposite.\n",
    "\"\"\"\n",
    "\n",
    "test_question = \"\"\"\n",
    "How should CMOs balance short-term performance demands with long-term brand \n",
    "building in today's data-driven marketing environment?\n",
    "\"\"\"\n",
    "\n",
    "test_media_outlet = \"Marketing Week\"\n",
    "test_journalist = \"Rachel Morrison\"\n",
    "\n",
    "def test_sync_generation(agent):\n",
    "    \"\"\"Test synchronous comment generation.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\" Skipping test - agent not initialized\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Testing Synchronous Comment Generation...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get available profiles\n",
    "    profiles = agent.profile_manager.list_profiles()\n",
    "    if len(profiles) == 0:\n",
    "        print(\" No profiles available for testing\")\n",
    "        return None\n",
    "    \n",
    "    test_executive = profiles[0]\n",
    "    print(f\"Testing with executive: {test_executive}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result = agent.generate_comment(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive,\n",
    "            journalist_name=test_journalist,\n",
    "            article_url=\"https://example.com/test-article\"\n",
    "        )\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\" Comment generated in {duration:.2f} seconds\")\n",
    "        \n",
    "        # Validate result structure\n",
    "        required_keys = [\n",
    "            'drafted_comment', 'humanized_comment', 'executive_profile',\n",
    "            'media_research', 'supporting_data', 'current_step', 'errors'\n",
    "        ]\n",
    "        \n",
    "        for key in required_keys:\n",
    "            assert key in result, f\"Missing key: {key}\"\n",
    "        print(\" Result structure valid\")\n",
    "        \n",
    "        # Check that comment was generated\n",
    "        assert result['drafted_comment'] is not None\n",
    "        assert len(result['drafted_comment']) > 0\n",
    "        print(f\" Drafted comment generated ({len(result['drafted_comment'])} chars)\")\n",
    "        \n",
    "        assert result['humanized_comment'] is not None\n",
    "        assert len(result['humanized_comment']) > 0\n",
    "        print(f\" Humanized comment generated ({len(result['humanized_comment'])} chars)\")\n",
    "        \n",
    "        # Check workflow completion\n",
    "        assert result['current_step'] in ['email_sent', 'completed']\n",
    "        print(f\" Workflow completed (step: {result['current_step']})\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"GENERATED COMMENT (HUMANIZED)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(result['humanized_comment'])\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\nEmail sent: {result['email_sent']}\")\n",
    "        print(f\"Errors: {len(result['errors'])} - {result['errors'] if result['errors'] else 'None'}\")\n",
    "        \n",
    "        print(\"\\n Synchronous generation tests passed\\n\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Comment generation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "sync_result = test_sync_generation(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Asynchronous Comment Generation\n",
    "\n",
    "Test async workflow with parallel operations for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_async_generation(agent):\n",
    "    \"\"\"Test asynchronous comment generation.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\" Skipping test - agent not initialized\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Testing Asynchronous Comment Generation...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    profiles = agent.profile_manager.list_profiles()\n",
    "    if len(profiles) == 0:\n",
    "        print(\" No profiles available for testing\")\n",
    "        return None\n",
    "    \n",
    "    test_executive = profiles[0]\n",
    "    print(f\"Testing with executive: {test_executive}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result = await agent.generate_comment_async(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive,\n",
    "            journalist_name=test_journalist\n",
    "        )\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\" Async comment generated in {duration:.2f} seconds\")\n",
    "        \n",
    "        # Validate result\n",
    "        assert result['drafted_comment'] is not None\n",
    "        assert result['humanized_comment'] is not None\n",
    "        assert 'duration_seconds' in result\n",
    "        print(f\" Result valid (reported duration: {result['duration_seconds']:.2f}s)\")\n",
    "        \n",
    "        # Check parallel research worked\n",
    "        assert result['media_research'] is not None\n",
    "        assert result['supporting_data'] is not None\n",
    "        print(\" Parallel research completed\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ASYNC GENERATED COMMENT\")\n",
    "        print(\"=\" * 80)\n",
    "        print(result['humanized_comment'][:500] + \"...\" if len(result['humanized_comment']) > 500 else result['humanized_comment'])\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"\\n Asynchronous generation tests passed\\n\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Async generation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run async test\n",
    "async_result = await test_async_generation(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Streaming Comment Generation\n",
    "\n",
    "Test real-time streaming of comment generation with progress updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_streaming_generation(agent):\n",
    "    \"\"\"Test streaming comment generation.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\" Skipping test - agent not initialized\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Testing Streaming Comment Generation...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    profiles = agent.profile_manager.list_profiles()\n",
    "    if len(profiles) == 0:\n",
    "        print(\" No profiles available for testing\")\n",
    "        return None\n",
    "    \n",
    "    test_executive = profiles[0]\n",
    "    print(f\"Testing with executive: {test_executive}\\n\")\n",
    "    \n",
    "    try:\n",
    "        events_received = []\n",
    "        final_result = None\n",
    "        \n",
    "        async for event in agent.generate_comment_stream(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive\n",
    "        ):\n",
    "            events_received.append(event)\n",
    "            \n",
    "            if event['event'] == 'started':\n",
    "                print(f\"â†’ Started: {event['step']}\")\n",
    "            elif event['event'] == 'completed':\n",
    "                print(f\" Completed: {event['step']}\")\n",
    "            elif event['event'] == 'streaming':\n",
    "                # Show first chunk of each step\n",
    "                if len([e for e in events_received if e.get('step') == event['step'] and e.get('event') == 'streaming']) == 1:\n",
    "                    print(f\"  Streaming {event['step']}...\", end='')\n",
    "            elif event['event'] == 'finished':\n",
    "                final_result = event['data']\n",
    "                print(f\"\\n Workflow finished\")\n",
    "        \n",
    "        print(f\"\\n Received {len(events_received)} events\")\n",
    "        \n",
    "        # Validate event sequence\n",
    "        expected_steps = ['profile', 'research', 'drafting', 'humanizing']\n",
    "        for step in expected_steps:\n",
    "            started = any(e['event'] == 'started' and e['step'] == step for e in events_received)\n",
    "            completed = any(e['event'] == 'completed' and e['step'] == step for e in events_received)\n",
    "            assert started and completed, f\"Missing events for step: {step}\"\n",
    "        print(\" All expected steps present\")\n",
    "        \n",
    "        # Check final result\n",
    "        assert final_result is not None\n",
    "        assert final_result['humanized_comment'] is not None\n",
    "        print(\" Final result valid\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STREAMED COMMENT (FINAL)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(final_result['humanized_comment'][:500] + \"...\" if len(final_result['humanized_comment']) > 500 else final_result['humanized_comment'])\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"\\n Streaming generation tests passed\\n\")\n",
    "        return final_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Streaming generation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run streaming test\n",
    "streaming_result = await test_streaming_generation(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Phase 3 Features (Memory, RAG, Evaluation)\n",
    "\n",
    "Test advanced Phase 3 features if enabled in configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_phase3_features(agent):\n",
    "    \"\"\"Test Phase 3 features: memory, RAG, and evaluation.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\" Skipping test - agent not initialized\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Testing Phase 3 Features...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check if Phase 3 components are enabled\n",
    "    memory_enabled = agent.memory and agent.memory.enabled\n",
    "    rag_enabled = agent.rag and agent.rag.enabled\n",
    "    evaluator_enabled = agent.evaluator and agent.evaluator.enabled\n",
    "    \n",
    "    print(f\"Memory enabled: {memory_enabled}\")\n",
    "    print(f\"RAG enabled: {rag_enabled}\")\n",
    "    print(f\"Evaluator enabled: {evaluator_enabled}\")\n",
    "    \n",
    "    if not (memory_enabled or rag_enabled or evaluator_enabled):\n",
    "        print(\"\\n No Phase 3 features enabled - skipping advanced tests\")\n",
    "        return None\n",
    "    \n",
    "    profiles = agent.profile_manager.list_profiles()\n",
    "    if len(profiles) == 0:\n",
    "        print(\" No profiles available for testing\")\n",
    "        return None\n",
    "    \n",
    "    test_executive = profiles[0]\n",
    "    session_id = f\"test_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    print(f\"\\nTesting with executive: {test_executive}\")\n",
    "    print(f\"Session ID: {session_id}\\n\")\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result = await agent.generate_comment_with_memory_and_evaluation(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive,\n",
    "            session_id=session_id,\n",
    "            enable_evaluation=True\n",
    "        )\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\" Phase 3 workflow completed in {duration:.2f} seconds\")\n",
    "        \n",
    "        # Validate Phase 3 result structure\n",
    "        assert 'session_id' in result\n",
    "        assert 'past_comments' in result\n",
    "        assert 'rag_context' in result\n",
    "        assert 'evaluation_scores' in result\n",
    "        assert 'phase3_enabled' in result\n",
    "        print(\" Phase 3 result structure valid\")\n",
    "        \n",
    "        # Display Phase 3 data\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PHASE 3 RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if memory_enabled:\n",
    "            print(f\"Past comments retrieved: {len(result['past_comments'])}\")\n",
    "            print(f\"Conversation history: {len(result.get('conversation_history', []))}\")\n",
    "        \n",
    "        if rag_enabled:\n",
    "            print(f\"RAG context: {result['rag_context'].get('enabled', False)}\")\n",
    "            if result['rag_context'].get('enabled'):\n",
    "                retrieval_counts = result['rag_context'].get('retrieval_counts', {})\n",
    "                print(f\"  - Similar comments: {retrieval_counts.get('similar_comments', 0)}\")\n",
    "                print(f\"  - Media knowledge: {retrieval_counts.get('media_knowledge', 0)}\")\n",
    "                print(f\"  - Examples: {retrieval_counts.get('examples', 0)}\")\n",
    "        \n",
    "        if evaluator_enabled:\n",
    "            eval_scores = result['evaluation_scores']\n",
    "            if eval_scores.get('enabled'):\n",
    "                print(f\"Evaluation scores:\")\n",
    "                print(f\"  - Overall score: {eval_scores.get('overall_score', 0):.2f}\")\n",
    "                print(f\"  - Overall passed: {eval_scores.get('overall_passed', False)}\")\n",
    "                if 'criteria_scores' in eval_scores:\n",
    "                    for criterion, score in eval_scores['criteria_scores'].items():\n",
    "                        print(f\"  - {criterion}: {score:.2f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Test Phase 3 stats\n",
    "        stats = agent.get_phase3_stats()\n",
    "        print(\"\\nPhase 3 Statistics:\")\n",
    "        print(json.dumps(stats, indent=2))\n",
    "        \n",
    "        print(\"\\n Phase 3 feature tests passed\\n\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Phase 3 workflow failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run Phase 3 test\n",
    "phase3_result = await test_phase3_features(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Error Handling and Edge Cases\n",
    "\n",
    "Test resilience and error handling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_handling(agent):\n",
    "    \"\"\"Test error handling and validation.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\" Skipping test - agent not initialized\")\n",
    "        return\n",
    "    \n",
    "    print(\"Testing Error Handling...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    profiles = agent.profile_manager.list_profiles()\n",
    "    test_executive = profiles[0] if profiles else \"Test Executive\"\n",
    "    \n",
    "    # Test 8.1: Empty article text\n",
    "    try:\n",
    "        agent.generate_comment(\n",
    "            article_text=\"\",\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive\n",
    "        )\n",
    "        print(\" Should have raised ValueError for empty article\")\n",
    "    except ValueError as e:\n",
    "        print(f\" Empty article validation: {str(e)[:50]}\")\n",
    "    \n",
    "    # Test 8.2: Empty question\n",
    "    try:\n",
    "        agent.generate_comment(\n",
    "            article_text=test_article,\n",
    "            journalist_question=\"\",\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive\n",
    "        )\n",
    "        print(\" Should have raised ValueError for empty question\")\n",
    "    except ValueError as e:\n",
    "        print(f\" Empty question validation: {str(e)[:50]}\")\n",
    "    \n",
    "    # Test 8.3: Invalid URL\n",
    "    try:\n",
    "        agent.generate_comment(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive,\n",
    "            article_url=\"not-a-valid-url\"\n",
    "        )\n",
    "        print(\" Should have raised ValueError for invalid URL\")\n",
    "    except ValueError as e:\n",
    "        print(f\" Invalid URL validation: {str(e)[:50]}\")\n",
    "    \n",
    "    # Test 8.4: Invalid email\n",
    "    try:\n",
    "        agent.generate_comment(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive,\n",
    "            pr_manager_email=\"not-an-email\"\n",
    "        )\n",
    "        print(\" Should have raised ValueError for invalid email\")\n",
    "    except ValueError as e:\n",
    "        print(f\" Invalid email validation: {str(e)[:50]}\")\n",
    "    \n",
    "    # Test 8.5: Article too long\n",
    "    try:\n",
    "        long_article = \"x\" * 60000\n",
    "        agent.generate_comment(\n",
    "            article_text=long_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive\n",
    "        )\n",
    "        print(\" Should have raised ValueError for article too long\")\n",
    "    except ValueError as e:\n",
    "        print(f\" Article length validation: {str(e)[:50]}\")\n",
    "    \n",
    "    # Test 8.6: Nonexistent profile (should fail)\n",
    "    try:\n",
    "        agent.generate_comment(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=\"Nonexistent Executive 12345\"\n",
    "        )\n",
    "        print(\" Workflow continued with nonexistent profile (unexpected)\")\n",
    "    except Exception as e:\n",
    "        print(f\" Nonexistent profile handled: {type(e).__name__}\")\n",
    "    \n",
    "    print(\"\\n Error handling tests passed\\n\")\n",
    "\n",
    "test_error_handling(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Cache Testing\n",
    "\n",
    "Test caching behavior to ensure responses are cached correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_caching(agent):\n",
    "    \"\"\"Test response caching.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\" Skipping test - agent not initialized\")\n",
    "        return\n",
    "    \n",
    "    if not agent.cache.enabled:\n",
    "        print(\" Cache is disabled - skipping cache tests\")\n",
    "        return\n",
    "    \n",
    "    print(\"Testing Cache Behavior...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    profiles = agent.profile_manager.list_profiles()\n",
    "    if len(profiles) == 0:\n",
    "        print(\" No profiles available for testing\")\n",
    "        return\n",
    "    \n",
    "    test_executive = profiles[0]\n",
    "    \n",
    "    # Note: Cache will be used if available, testing speedup\n",
    "    try:\n",
    "        # Test 9.1: First request (cache miss)\n",
    "        print(\"First request (should be cache miss)...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result1 = agent.generate_comment(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive\n",
    "        )\n",
    "        \n",
    "        duration1 = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\" First request completed in {duration1:.2f}s\")\n",
    "        \n",
    "        # Test 9.2: Second request (cache hit)\n",
    "        print(\"\\nSecond request (should be cache hit)...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result2 = agent.generate_comment(\n",
    "            article_text=test_article,\n",
    "            journalist_question=test_question,\n",
    "            media_outlet=test_media_outlet,\n",
    "            executive_name=test_executive\n",
    "        )\n",
    "        \n",
    "        duration2 = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\" Second request completed in {duration2:.2f}s\")\n",
    "        \n",
    "        # Test 9.3: Verify cache speedup\n",
    "        if duration2 < duration1 * 0.5:  # Should be at least 2x faster\n",
    "            print(f\" Cache speedup: {duration1/duration2:.1f}x faster\")\n",
    "        else:\n",
    "            print(f\" Cache may not be working (speedup: {duration1/duration2:.1f}x)\")\n",
    "        \n",
    "        # Test 9.4: Verify same result\n",
    "        assert result1['humanized_comment'] == result2['humanized_comment']\n",
    "        print(\" Cached result matches original\")\n",
    "        \n",
    "        print(\"\\n Cache tests passed\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Cache test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "test_caching(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Summary\n",
    "\n",
    "Display a comprehensive summary of all test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_test_summary():\n",
    "    \"\"\"Display summary of all tests.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    tests = [\n",
    "        (\"Configuration Validation\", agent is not None),\n",
    "        (\"Profile Management\", agent is not None),\n",
    "        (\"Agent Initialization\", agent is not None),\n",
    "        (\"Synchronous Generation\", sync_result is not None),\n",
    "        (\"Asynchronous Generation\", async_result is not None),\n",
    "        (\"Streaming Generation\", streaming_result is not None),\n",
    "        (\"Phase 3 Features\", phase3_result is not None),\n",
    "        (\"Error Handling\", True),  # Always runs\n",
    "        (\"Cache Testing\", agent is not None and agent.cache.enabled)\n",
    "    ]\n",
    "    \n",
    "    passed = sum(1 for _, result in tests if result)\n",
    "    total = len(tests)\n",
    "    \n",
    "    for test_name, result in tests:\n",
    "        status = \" PASS\" if result else \" FAIL/SKIP\"\n",
    "        print(f\"{status:12} {test_name}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Results: {passed}/{total} tests passed ({passed/total*100:.0f}%)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if passed == total:\n",
    "        print(\"\\nðŸŽ‰ All tests passed! The PR Agent System is working correctly.\")\n",
    "    else:\n",
    "        print(f\"\\n {total - passed} test(s) failed or were skipped. Review output above.\")\n",
    "\n",
    "display_test_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Testing\n",
    "\n",
    "Use the cells below for ad-hoc testing and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test: Try your own article and question\n",
    "\n",
    "custom_article = \"\"\"\n",
    "Paste your article text here...\n",
    "\"\"\"\n",
    "\n",
    "custom_question = \"\"\"\n",
    "Paste your journalist question here...\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment and run to test with custom input\n",
    "# if agent and custom_article.strip() and custom_question.strip():\n",
    "#     custom_result = agent.generate_comment(\n",
    "#         article_text=custom_article,\n",
    "#         journalist_question=custom_question,\n",
    "#         media_outlet=\"Your Media Outlet\",\n",
    "#         executive_name=agent.profile_manager.list_profiles()[0]\n",
    "#     )\n",
    "#     print(custom_result['humanized_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect agent configuration\n",
    "if agent:\n",
    "    print(\"Current Agent Configuration:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {agent.config.model_name}\")\n",
    "    print(f\"Temperature: {agent.config.temperature}\")\n",
    "    print(f\"Humanizer Temperature: {agent.config.humanizer_temperature}\")\n",
    "    print(f\"Max Search Results: {agent.config.max_search_results}\")\n",
    "    print(f\"Cache Enabled: {agent.cache.enabled}\")\n",
    "    print(f\"Streaming Enabled: {agent.config.enable_streaming}\")\n",
    "    print(f\"Async Enabled: {agent.config.async_enabled}\")\n",
    "    print(f\"Verbose Logging: {agent.config.enable_verbose_logging}\")\n",
    "    print(f\"Tracing Enabled: {agent.config.enable_tracing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up resources after testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup code (if needed)\n",
    "print(\"Test suite completed successfully!\")\n",
    "print(\"\\nTo run specific tests again, simply re-execute the relevant cells above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}